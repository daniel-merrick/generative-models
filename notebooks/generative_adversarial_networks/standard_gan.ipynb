{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cifar10(data_path='./data'):\n",
    "    \"\"\"\n",
    "    Download CIFAR-10 dataset and return trainset, testset, and classes\n",
    "\n",
    "    Apply basic transformations to the data to normalize it between [-1, 1]\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {len(trainset)}\")\n",
    "    print(f\"Test set size: {len(testset)}\")\n",
    "    \n",
    "    # CIFAR-10 classes\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "              'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainset, testset, classes\n",
    "\n",
    "def get_dataloader(trainset, testset, batch_size=128):\n",
    "    \"\"\"Create DataLoader objects for training and testing\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_vector_len = 100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_vector_len = noise_vector_len\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_vector_len, 512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        random_samples = torch.randn((batch_size, self.noise_vector_len, 1, 1)).to(\"cuda\")\n",
    "        fake_samples = self.model(random_samples)\n",
    "        return fake_samples\n",
    "    \n",
    "    def loss(self, adversary_output_on_fake):\n",
    "        \"\"\"\n",
    "        Generator aims to maximize adversary output on fake samples\n",
    "        (or minimize negative log of adversary output)\n",
    "        \"\"\"\n",
    "        return F.binary_cross_entropy(\n",
    "            adversary_output_on_fake,\n",
    "            torch.ones_like(adversary_output_on_fake)\n",
    "        )\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def loss(self, output_on_real, output_on_fake):\n",
    "        \"\"\"\n",
    "        Adversary aims to:\n",
    "        - Maximize output on real samples (close to 1)\n",
    "        - Minimize output on fake samples (close to 0)\n",
    "        \"\"\"\n",
    "        # Real samples should be classified as 1\n",
    "        loss_on_real = F.binary_cross_entropy(\n",
    "            output_on_real,\n",
    "            torch.ones_like(output_on_real)\n",
    "        )\n",
    "        \n",
    "        # Fake samples should be classified as 0\n",
    "        loss_on_fake = F.binary_cross_entropy(\n",
    "            output_on_fake,\n",
    "            torch.zeros_like(output_on_fake)\n",
    "        )\n",
    "        \n",
    "        # Total loss is the average of both\n",
    "        return (loss_on_real + loss_on_fake) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_samples(generator, writer, epoch):\n",
    "    \"\"\"Save original and reconstructed images to tensorboard\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_batch = generator(batch_size=32)\n",
    "        data_cpu = generated_batch[:16].cpu()\n",
    "        writer.add_images('Generated', data_cpu, epoch)\n",
    "\n",
    "def train_epoch(generator, adversary, train_loader, generator_optimizer, adversary_optimizer, device, writer, epoch):\n",
    "    generator.train()\n",
    "    adversary.train()\n",
    "\n",
    "    epoch_g_loss = 0\n",
    "    epoch_a_loss = 0\n",
    "    n_samples = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        batch_size = data.size(0)\n",
    "        real_images = data.to(device)\n",
    "\n",
    "        # Train Adversary\n",
    "        adversary_optimizer.zero_grad()\n",
    "        real_predictions = adversary(real_images)\n",
    "        \n",
    "        fake_images = generator(batch_size)\n",
    "        fake_predictions = adversary(fake_images.detach())\n",
    "        a_loss = adversary.loss(real_predictions, fake_predictions)\n",
    "        a_loss.backward()\n",
    "        adversary_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        fake_images = generator(batch_size)\n",
    "        fake_predictions = adversary(fake_images)\n",
    "        g_loss = generator.loss(fake_predictions)\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch}] Batch [{batch_idx}/{len(train_loader)}] '\n",
    "                      f'A_Loss: {a_loss.item():.4f} G_Loss: {g_loss.item():.4f}')\n",
    "        \n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_a_loss += a_loss.item()\n",
    "    \n",
    "    save_image_samples(generator, writer, epoch)  # Using the last batch\n",
    "    \n",
    "\n",
    "    avg_g_loss = epoch_g_loss / n_samples\n",
    "    avg_a_loss = epoch_a_loss / n_samples\n",
    "    \n",
    "    writer.add_scalar('Loss/train/g_loss', avg_g_loss, epoch)\n",
    "    writer.add_scalar('Loss/train/a_loss', avg_a_loss, epoch)\n",
    "    return avg_g_loss, avg_a_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(epochs=100, batch_size=128, learning_rate=1e-3, device=\"cuda\"):\n",
    "    # Get data\n",
    "    trainset, testset, _ = download_cifar10()\n",
    "    train_loader, test_loader = get_dataloader(trainset, testset, batch_size)\n",
    "    \n",
    "    generator = Generator().to(device)\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=3e-3)\n",
    "\n",
    "    adversary = Adversary().to(device)\n",
    "    adversary_optimizer = torch.optim.Adam(adversary.parameters(), lr=1e-3)\n",
    "\n",
    "    log_dir = f'runs/VAE_CIFAR10_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        g_loss, a_loss = train_epoch(generator, adversary, train_loader, generator_optimizer, adversary_optimizer, device, writer, epoch)\n",
    "        \n",
    "        # Save a checkpoint every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            if not os.path.exists(f'{log_dir}/models'):\n",
    "                os.makedirs(f'{log_dir}/models')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'g_model_state_dict': generator.state_dict(),\n",
    "                'a_model_state_dict': adversary.state_dict(),\n",
    "                'g_optimizer_state_dict': generator_optimizer.state_dict(),\n",
    "                'a_optimizer_state_dict': adversary_optimizer.state_dict(),\n",
    "                'g_loss': g_loss,\n",
    "                'a_loss': a_loss,\n",
    "            }, f'{log_dir}/models/vae_checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(epochs=100, batch_size=128, learning_rate=3e-3, device=\"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
