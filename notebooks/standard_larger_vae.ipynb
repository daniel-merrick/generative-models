{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is a simple VAE model that uses a convolutional encoder and decoder to learn a latent representation of the input image. \n",
    "\n",
    "It's a standard VAE with no tricks so it doesn't work well, but good for learning the basics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for downloading CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cifar10(data_path='./data'):\n",
    "    \"\"\"\n",
    "    Download CIFAR-10 dataset and return trainset, testset, and classes\n",
    "\n",
    "    Apply basic transformations to the data to normalize it between [-1, 1]\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {len(trainset)}\")\n",
    "    print(f\"Test set size: {len(testset)}\")\n",
    "    \n",
    "    # CIFAR-10 classes\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "              'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainset, testset, classes\n",
    "\n",
    "def get_dataloader(trainset, testset, batch_size=128):\n",
    "    \"\"\"Create DataLoader objects for training and testing\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder, trained to estimate p(z|x) from the input image(s) p(x)\n",
    "        # p(z|x) is the conditional latent distribution\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),   # [b, 64, 16, 16]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # [b, 128, 8, 8]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # [b, 256, 4, 4]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Latent space, aka the estimated p(z|x) or the conditional latent distribution\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_var = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        # Decoder, used to estimate the conditional distribution of the input image p(x|z)\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "         \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x) # p(z|x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "           \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 256, 4, 4)\n",
    "        x = self.decoder(x) # p(x|z)\n",
    "        return x\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from the latent distribution while allowing backpropagation\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, log_var, beta=2.):\n",
    "    # Reconstruction loss:\n",
    "    # p(x|z) = N(x; μ(z), σ²I)\n",
    "    # log p(x|z) = -0.5 * (log(2πσ²) + (x - μ(z))²/σ²)\n",
    "    # log p(x|z) ∝ -0.5 * Σ(x - μ(z))²\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence loss\n",
    "    # KL(N(μ,σ²) || N(0,1)) = 0.5 * (μ² + σ² - ln(σ²) - 1)\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) * beta\n",
    "\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "def save_image_samples(model, data, writer, epoch, device):\n",
    "    \"\"\"Save original and reconstructed images to tensorboard\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get reconstructions\n",
    "        data = data.to(device)\n",
    "        recon_batch, _, _ = model(data)\n",
    "        \n",
    "        data_cpu = data[:8].cpu()\n",
    "        recon_cpu = recon_batch.cpu()[:8]\n",
    "        comparison = torch.cat([\n",
    "            data_cpu,  # Original images\n",
    "            recon_cpu  # Reconstructed images\n",
    "        ])\n",
    "        \n",
    "        # Add images to tensorboard\n",
    "        writer.add_images('Original_Reconstructed', comparison, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, writer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    n_samples = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss, recon_loss, kl_loss = vae_loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    save_image_samples(model, data, writer, epoch, device)  # Using the last batch\n",
    "    \n",
    "\n",
    "    avg_loss = train_loss / n_samples\n",
    "    avg_recon_loss = train_recon_loss / n_samples\n",
    "    avg_kl_loss = train_kl_loss / n_samples\n",
    "    \n",
    "    writer.add_scalar('Loss/train/total', avg_loss, epoch)\n",
    "    writer.add_scalar('Loss/train/reconstruction', avg_recon_loss, epoch)\n",
    "    writer.add_scalar('Loss/train/kl_divergence', avg_kl_loss, epoch)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_vae(epochs=100, batch_size=128, learning_rate=1e-3):\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    # Get data\n",
    "    trainset, testset, _ = download_cifar10()  # Using your existing function\n",
    "    train_loader, test_loader = get_dataloader(trainset, testset, batch_size)\n",
    "    \n",
    "    # Initialize model, optimizer, and tensorboard\n",
    "    model = VAE().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    log_dir = f'runs/VAE_CIFAR10_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, DEVICE, writer, epoch)\n",
    "        \n",
    "        # Save a checkpoint every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            if not os.path.exists(f'{log_dir}/models'):\n",
    "                os.makedirs(f'{log_dir}/models')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, f'{log_dir}/models/vae_checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set size: 50000\n",
      "Test set size: 10000\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1874.945190\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 358.129547\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 306.650238\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 285.045288\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 274.549164\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 275.364990\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 268.307251\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 253.626770\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 279.568604\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 278.679688\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 267.963989\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 260.339600\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 266.762329\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 258.582947\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 259.330017\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 248.458344\n"
     ]
    }
   ],
   "source": [
    "train_vae(epochs=200, batch_size=128, learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_display_samples(log_dir, epoch=100, num_samples=64, latent_dim=128, device='cuda'):\n",
    "    \"\"\"Generate and display random samples from the VAE decoder\"\"\"\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VAE(latent_dim=128).to(DEVICE)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(f'{log_dir}/models/vae_checkpoint_epoch_{str(epoch).zfill(3)}.pt', weights_only=True)  # adjust epoch number as needed\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample random latent vectors\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        \n",
    "        # Generate images\n",
    "        samples = model.decode(z)\n",
    "        \n",
    "        # Denormalize images from [-1, 1] to [0, 1]\n",
    "        samples = (samples + 1) / 2\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        samples = samples.cpu()\n",
    "        \n",
    "        # Create a grid of images\n",
    "        fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "            # Convert from [C,H,W] to [H,W,C] format\n",
    "            img = samples[idx].permute(1, 2, 0)\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model instance\n",
    "# log_dir = './runs/VAE_CIFAR10_20241226-181029'\n",
    "# samples = generate_and_display_samples(log_dir, 200)\n",
    "\n",
    "# Optionally save to disk\n",
    "# torchvision.utils.save_image(samples, 'vae_samples.png', nrow=8, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
