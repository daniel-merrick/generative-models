{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical VAE\n",
    "\n",
    "This is a simple implementation of a Hierarchical VAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cifar10(data_path='./data'):\n",
    "    \"\"\"\n",
    "    Download CIFAR-10 dataset and return trainset, testset, and classes\n",
    "\n",
    "    Apply basic transformations to the data to normalize it between [-1, 1]\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_path,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {len(trainset)}\")\n",
    "    print(f\"Test set size: {len(testset)}\")\n",
    "    \n",
    "    # CIFAR-10 classes\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "              'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainset, testset, classes\n",
    "\n",
    "def get_dataloader(trainset, testset, batch_size=128):\n",
    "    \"\"\"Create DataLoader objects for training and testing\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hierarchical VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalVAE(nn.Module):\n",
    "    def __init__(self, latent_dims=[512, 256, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dims = latent_dims  # [z1: 512, z2: 256, z3: 128]\n",
    "        \n",
    "        # Encoder path\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),    # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),   # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()  # 64 * 8 * 8 = 4096\n",
    "        )\n",
    "        \n",
    "        # Latent variables means and logvars\n",
    "        self.z1_mu = nn.Linear(64 * 8 * 8, latent_dims[0])\n",
    "        self.z1_logvar = nn.Linear(64 * 8 * 8, latent_dims[0])\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[0], 512),  # z1: 512 -> hidden: 512\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.z2_mu = nn.Linear(512, latent_dims[1])\n",
    "        self.z2_logvar = nn.Linear(512, latent_dims[1])\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[1], 256),  # z2: 256 -> hidden: 256\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.z3_mu = nn.Linear(256, latent_dims[2])\n",
    "        self.z3_logvar = nn.Linear(256, latent_dims[2])\n",
    "        \n",
    "        # Decoder path\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[2], latent_dims[1]),  # z3: 128 -> hidden: 256\n",
    "            nn.BatchNorm1d(latent_dims[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Add mixing layers for z2 and h3\n",
    "        self.mix2 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[1] * 2, latent_dims[1]),  # (256 + 256) -> 256\n",
    "            nn.BatchNorm1d(latent_dims[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[1], latent_dims[0]),  # z2: 256 -> hidden: 512\n",
    "            nn.BatchNorm1d(latent_dims[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Add mixing layers for z1 and h2\n",
    "        self.mix1 = nn.Sequential(\n",
    "            nn.Linear(latent_dims[0] * 2, latent_dims[0]),  # (512 + 512) -> 512\n",
    "            nn.BatchNorm1d(latent_dims[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder1_fc = nn.Sequential(\n",
    "            nn.Linear(latent_dims[0], 64 * 8 * 8),  # z1: 512 -> hidden: 4096\n",
    "            nn.BatchNorm1d(64 * 8 * 8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder1_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),   # 16x16 -> 32x32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes the input into hierarchical latent variables.\"\"\"\n",
    "        # Level 1\n",
    "        h1 = self.encoder1(x)\n",
    "        z1_mu = self.z1_mu(h1)\n",
    "        z1_logvar = self.z1_logvar(h1)\n",
    "        z1 = self.reparameterize(z1_mu, z1_logvar)\n",
    "\n",
    "        # Level 2\n",
    "        h2_input = z1\n",
    "        h2 = self.encoder2(h2_input)\n",
    "        z2_mu = self.z2_mu(h2)\n",
    "        z2_logvar = self.z2_logvar(h2)\n",
    "        z2 = self.reparameterize(z2_mu, z2_logvar)\n",
    "        \n",
    "        # Level 3\n",
    "        h3_input = z2\n",
    "        h3 = self.encoder3(h3_input)\n",
    "        z3_mu = self.z3_mu(h3)\n",
    "        z3_logvar = self.z3_logvar(h3)\n",
    "        z3 = self.reparameterize(z3_mu, z3_logvar)\n",
    "        \n",
    "        return [z3, z2, z1], [(z3_mu, z3_logvar), \n",
    "                             (z2_mu, z2_logvar), \n",
    "                             (z1_mu, z1_logvar)]\n",
    "\n",
    "    def decode(self, zs):\n",
    "        z3, z2, z1 = zs\n",
    "\n",
    "        h3 = self.decoder3(z3)  # [batch_size, 256]\n",
    "\n",
    "        combined_z2 = self.mix2(torch.cat([z2, h3], dim=1))\n",
    "        h2 = self.decoder2(combined_z2)  # [batch_size, 512]\n",
    "\n",
    "        combined_z1 = self.mix1(torch.cat([z1, h2], dim=1))\n",
    "        h1_flat = self.decoder1_fc(combined_z1)  # [batch_size, 4096]\n",
    "        \n",
    "        # Reconstruct the image\n",
    "        h1 = self.decoder1_conv(h1_flat.view(-1, 64, 8, 8))  # [batch_size, 3, 32, 32]\n",
    "\n",
    "        return h1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the HVAE.\"\"\"\n",
    "        zs, mu_vars = self.encode(x)\n",
    "        recon_x = self.decode(zs)\n",
    "        return recon_x, mu_vars\n",
    "\n",
    "    def random_samples(self, num_samples, device='cuda'):\n",
    "        \"\"\"Generates random samples from the HVAE using conditional sampling.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Sample z1 (lowest level) from prior\n",
    "            z1 = torch.randn(num_samples, self.latent_dims[0]).to(device)\n",
    "            \n",
    "            # Generate z2 by encoding z1\n",
    "            h2 = self.encoder2(z1)\n",
    "            z2_mu = self.z2_mu(h2)\n",
    "            z2_logvar = self.z2_logvar(h2)\n",
    "            z2 = self.reparameterize(z2_mu, z2_logvar)\n",
    "            \n",
    "            # Generate z3 by encoding z2\n",
    "            h3 = self.encoder3(z2)\n",
    "            z3_mu = self.z3_mu(h3)\n",
    "            z3_logvar = self.z3_logvar(h3)\n",
    "            z3 = self.reparameterize(z3_mu, z3_logvar)\n",
    "            \n",
    "            # Decode all latent variables\n",
    "            samples = self.decode([z3, z2, z1])\n",
    "            samples = (samples + 1) / 2\n",
    "            return samples.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvae_loss_function(recon_x, x, mu_vars, beta=1., epoch=None, warmup_epochs=50):\n",
    "    # Reconstruction loss:\n",
    "    # p(x|z) = N(x; μ(z), σ²I)\n",
    "    # log p(x|z) = -0.5 * (log(2πσ²) + (x - μ(z))²/σ²)\n",
    "    # log p(x|z) ∝ -0.5 * Σ(x - μ(z))²\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence loss\n",
    "    # KL(N(μ,σ²) || N(0,1)) = 0.5 * (μ² + σ² - ln(σ²) - 1)\n",
    "    kl_losses = []\n",
    "    kl_weights = [1, .7, .5]\n",
    "    for (mu, log_var), weight in zip(mu_vars, kl_weights):\n",
    "        kl_i = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        kl_losses.append(kl_i * weight)\n",
    "    total_kl_loss = torch.stack(kl_losses).mean()\n",
    " \n",
    "\n",
    "    # Apply KL annealing if epoch is provided:\n",
    "    # This is useful because usually the recon_loss\n",
    "    # overwhelms the optimizer and we end up in a posterior collapse\n",
    "    # where the KL Divergence never decreases\n",
    "    # This is a simple way to gradually increase the KL loss\n",
    "    # and prevent posterior collapse\n",
    "    if epoch is not None:\n",
    "        # Linearly increase beta from 0 to its final value\n",
    "        beta_weight = min(epoch / warmup_epochs, 1.0) * beta\n",
    "    else:\n",
    "        beta_weight = beta\n",
    "\n",
    "    total_loss = recon_loss + beta_weight*total_kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss, total_kl_loss, kl_losses\n",
    "\n",
    "def save_image_samples(model, data, writer, epoch, device):\n",
    "    \"\"\"Save original and reconstructed images to tensorboard\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get reconstructions\n",
    "        data = data.to(device)\n",
    "        recon_batch, _ = model(data)\n",
    "        \n",
    "        data_cpu = data[:8].cpu()\n",
    "        recon_cpu = recon_batch.cpu()[:8]\n",
    "        comparison = torch.cat([\n",
    "            data_cpu,\n",
    "            recon_cpu\n",
    "        ])\n",
    "        \n",
    "        # Add images to tensorboard\n",
    "        writer.add_images('Original_Reconstructed', comparison, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, writer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    # train_kl_losses = [0, 0, 0]  # For each level\n",
    "    train_kl_loss = 0\n",
    "    n_samples = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu_vars = model(data)  # HVAE returns mu_vars list\n",
    "        loss, recon_loss, total_kl, kl_losses = hvae_loss_function(\n",
    "            recon_batch, data, mu_vars, epoch=epoch\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += total_kl.item()\n",
    "        # for i, kl in enumerate(kl_losses):\n",
    "        #     train_kl_losses[i] += kl.item()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "    \n",
    "    save_image_samples(model, data, writer, epoch, device)\n",
    "\n",
    "    # Log metrics\n",
    "    avg_loss = train_loss / n_samples\n",
    "    avg_recon_loss = train_recon_loss / n_samples\n",
    "    avg_kl_loss = train_kl_loss / n_samples\n",
    "    \n",
    "    writer.add_scalar('Loss/train/total', avg_loss, epoch)\n",
    "    writer.add_scalar('Loss/train/reconstruction', avg_recon_loss, epoch)\n",
    "    writer.add_scalar('Loss/train/kl_divergence', avg_kl_loss, epoch)\n",
    "    # Log individual KL losses\n",
    "    # for i, kl in enumerate(avg_kl_losses):\n",
    "        # writer.add_scalar(f'Loss/train/kl_level_{i+1}', kl, epoch)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_vae(epochs=100, batch_size=128, learning_rate=1e-3, device=\"cuda\"):\n",
    "    # Get data\n",
    "    trainset, testset, _ = download_cifar10()  # Using your existing function\n",
    "    train_loader, test_loader = get_dataloader(trainset, testset, batch_size)\n",
    "    \n",
    "    # Initialize model, optimizer, and tensorboard\n",
    "    model = HierarchicalVAE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    log_dir = f'runs/HVAE_CIFAR10_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, writer, epoch)\n",
    "        \n",
    "        # Save a checkpoint every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            if not os.path.exists(f'{log_dir}/models'):\n",
    "                os.makedirs(f'{log_dir}/models')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, f'{log_dir}/models/hvae_checkpoint_epoch_{epoch}.pt')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set size: 50000\n",
      "Test set size: 10000\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1506.512207\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 213.966629\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 168.320374\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 146.051971\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 136.676590\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 127.524582\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 118.169853\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 102.050323\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 108.074844\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 99.339569\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 99.872574\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 95.560928\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 97.865570\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 90.686798\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 99.730988\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 90.292816\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 85.868813\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 96.005089\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 85.902908\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 92.140747\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 89.324829\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 77.986382\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 85.358406\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 82.223358\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 89.768219\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 84.666672\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 90.301971\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 81.731079\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 77.890106\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 79.359795\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 76.949417\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 82.609138\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 76.420349\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 75.626793\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 83.179008\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 76.656319\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 87.772354\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 79.864906\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 75.640938\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 73.903389\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 73.065598\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 77.471863\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 70.908218\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 74.201920\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 71.579643\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 71.218529\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 76.476074\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 74.052948\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 72.187996\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 77.027725\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 68.677826\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 75.653763\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 99.043510\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 70.914894\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 75.923225\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 67.241158\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 70.997063\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 73.979980\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 76.110886\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 70.441101\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 79.428581\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 70.031464\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 76.877808\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 70.578270\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 76.127182\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 75.549820\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 76.343895\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 80.203003\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 74.367058\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 77.056381\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 74.466309\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 71.331345\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 73.635719\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 73.386673\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 74.035599\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 75.419952\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 79.575714\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 74.212463\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 72.963799\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 71.118210\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 73.035080\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 75.057037\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 77.915123\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 79.467865\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 75.816727\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 76.766327\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 79.838852\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 71.476402\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 77.268677\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 77.310616\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 74.042557\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 75.547516\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 82.250717\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 81.929016\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 84.017632\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 79.955948\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 77.817589\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 76.647827\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 76.081383\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 84.884117\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 80.615364\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 80.461540\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 78.980324\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 76.535469\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 80.101410\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 79.608185\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 83.260185\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 78.222832\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 88.463043\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 83.687943\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 84.634254\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 82.075737\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 86.434509\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 79.816895\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 83.452835\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 81.197762\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 81.972900\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 81.288544\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 82.361435\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 80.757057\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 82.202019\n",
      "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 84.354034\n",
      "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 82.510818\n",
      "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 80.747147\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 87.203812\n",
      "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 79.804482\n",
      "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 84.059036\n",
      "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 83.057182\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 86.679253\n",
      "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 83.577988\n",
      "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 84.521988\n",
      "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 85.406799\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 86.736557\n",
      "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 88.545219\n",
      "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 85.265495\n",
      "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 88.491699\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 83.770142\n",
      "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 86.235397\n",
      "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 90.796417\n",
      "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 84.471489\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 85.727707\n",
      "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 85.997299\n",
      "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 89.030617\n",
      "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 83.493652\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 91.835663\n",
      "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 88.330383\n",
      "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 89.950470\n",
      "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 88.386398\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 96.722229\n",
      "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 91.887909\n",
      "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 89.331024\n",
      "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 95.855621\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 89.923943\n",
      "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 84.266174\n",
      "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 85.636765\n",
      "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 90.673088\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 85.838989\n",
      "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 89.384262\n",
      "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 87.862473\n",
      "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 84.900497\n",
      "Train Epoch: 41 [0/50000 (0%)]\tLoss: 94.763809\n",
      "Train Epoch: 41 [12800/50000 (26%)]\tLoss: 92.209351\n",
      "Train Epoch: 41 [25600/50000 (51%)]\tLoss: 88.780014\n",
      "Train Epoch: 41 [38400/50000 (77%)]\tLoss: 90.580765\n",
      "Train Epoch: 42 [0/50000 (0%)]\tLoss: 89.204163\n",
      "Train Epoch: 42 [12800/50000 (26%)]\tLoss: 89.492554\n",
      "Train Epoch: 42 [25600/50000 (51%)]\tLoss: 89.546463\n",
      "Train Epoch: 42 [38400/50000 (77%)]\tLoss: 87.342392\n",
      "Train Epoch: 43 [0/50000 (0%)]\tLoss: 95.050537\n",
      "Train Epoch: 43 [12800/50000 (26%)]\tLoss: 88.140915\n",
      "Train Epoch: 43 [25600/50000 (51%)]\tLoss: 94.592072\n",
      "Train Epoch: 43 [38400/50000 (77%)]\tLoss: 88.933594\n",
      "Train Epoch: 44 [0/50000 (0%)]\tLoss: 93.940430\n",
      "Train Epoch: 44 [12800/50000 (26%)]\tLoss: 93.710617\n",
      "Train Epoch: 44 [25600/50000 (51%)]\tLoss: 93.069168\n",
      "Train Epoch: 44 [38400/50000 (77%)]\tLoss: 95.549950\n",
      "Train Epoch: 45 [0/50000 (0%)]\tLoss: 90.964371\n",
      "Train Epoch: 45 [12800/50000 (26%)]\tLoss: 93.501198\n",
      "Train Epoch: 45 [25600/50000 (51%)]\tLoss: 91.893448\n",
      "Train Epoch: 45 [38400/50000 (77%)]\tLoss: 92.062363\n",
      "Train Epoch: 46 [0/50000 (0%)]\tLoss: 92.974815\n",
      "Train Epoch: 46 [12800/50000 (26%)]\tLoss: 93.297287\n",
      "Train Epoch: 46 [25600/50000 (51%)]\tLoss: 102.166603\n",
      "Train Epoch: 46 [38400/50000 (77%)]\tLoss: 88.392395\n",
      "Train Epoch: 47 [0/50000 (0%)]\tLoss: 95.727242\n",
      "Train Epoch: 47 [12800/50000 (26%)]\tLoss: 89.491806\n",
      "Train Epoch: 47 [25600/50000 (51%)]\tLoss: 93.284744\n",
      "Train Epoch: 47 [38400/50000 (77%)]\tLoss: 91.559006\n",
      "Train Epoch: 48 [0/50000 (0%)]\tLoss: 93.427063\n",
      "Train Epoch: 48 [12800/50000 (26%)]\tLoss: 94.603149\n",
      "Train Epoch: 48 [25600/50000 (51%)]\tLoss: 99.770935\n",
      "Train Epoch: 48 [38400/50000 (77%)]\tLoss: 99.911697\n",
      "Train Epoch: 49 [0/50000 (0%)]\tLoss: 95.066727\n",
      "Train Epoch: 49 [12800/50000 (26%)]\tLoss: 97.672241\n",
      "Train Epoch: 49 [25600/50000 (51%)]\tLoss: 91.052971\n",
      "Train Epoch: 49 [38400/50000 (77%)]\tLoss: 93.487770\n",
      "Train Epoch: 50 [0/50000 (0%)]\tLoss: 91.759613\n",
      "Train Epoch: 50 [12800/50000 (26%)]\tLoss: 91.168633\n",
      "Train Epoch: 50 [25600/50000 (51%)]\tLoss: 94.549530\n",
      "Train Epoch: 50 [38400/50000 (77%)]\tLoss: 96.989731\n",
      "Train Epoch: 51 [0/50000 (0%)]\tLoss: 96.745941\n",
      "Train Epoch: 51 [12800/50000 (26%)]\tLoss: 94.971558\n",
      "Train Epoch: 51 [25600/50000 (51%)]\tLoss: 98.054794\n",
      "Train Epoch: 51 [38400/50000 (77%)]\tLoss: 96.348907\n",
      "Train Epoch: 52 [0/50000 (0%)]\tLoss: 101.394821\n",
      "Train Epoch: 52 [12800/50000 (26%)]\tLoss: 98.805893\n",
      "Train Epoch: 52 [25600/50000 (51%)]\tLoss: 94.673325\n",
      "Train Epoch: 52 [38400/50000 (77%)]\tLoss: 93.395523\n",
      "Train Epoch: 53 [0/50000 (0%)]\tLoss: 92.576736\n",
      "Train Epoch: 53 [12800/50000 (26%)]\tLoss: 93.192963\n",
      "Train Epoch: 53 [25600/50000 (51%)]\tLoss: 95.158127\n",
      "Train Epoch: 53 [38400/50000 (77%)]\tLoss: 101.662628\n",
      "Train Epoch: 54 [0/50000 (0%)]\tLoss: 94.118752\n",
      "Train Epoch: 54 [12800/50000 (26%)]\tLoss: 99.756714\n",
      "Train Epoch: 54 [25600/50000 (51%)]\tLoss: 93.363739\n",
      "Train Epoch: 54 [38400/50000 (77%)]\tLoss: 93.101318\n",
      "Train Epoch: 55 [0/50000 (0%)]\tLoss: 93.620819\n",
      "Train Epoch: 55 [12800/50000 (26%)]\tLoss: 94.823906\n",
      "Train Epoch: 55 [25600/50000 (51%)]\tLoss: 97.070000\n",
      "Train Epoch: 55 [38400/50000 (77%)]\tLoss: 94.582397\n",
      "Train Epoch: 56 [0/50000 (0%)]\tLoss: 95.716713\n",
      "Train Epoch: 56 [12800/50000 (26%)]\tLoss: 94.935997\n",
      "Train Epoch: 56 [25600/50000 (51%)]\tLoss: 94.938881\n",
      "Train Epoch: 56 [38400/50000 (77%)]\tLoss: 95.313667\n",
      "Train Epoch: 57 [0/50000 (0%)]\tLoss: 95.962997\n",
      "Train Epoch: 57 [12800/50000 (26%)]\tLoss: 93.013107\n",
      "Train Epoch: 57 [25600/50000 (51%)]\tLoss: 95.196991\n",
      "Train Epoch: 57 [38400/50000 (77%)]\tLoss: 95.015152\n",
      "Train Epoch: 58 [0/50000 (0%)]\tLoss: 93.103661\n",
      "Train Epoch: 58 [12800/50000 (26%)]\tLoss: 94.981926\n",
      "Train Epoch: 58 [25600/50000 (51%)]\tLoss: 93.689812\n",
      "Train Epoch: 58 [38400/50000 (77%)]\tLoss: 93.931000\n",
      "Train Epoch: 59 [0/50000 (0%)]\tLoss: 98.859123\n",
      "Train Epoch: 59 [12800/50000 (26%)]\tLoss: 101.132607\n",
      "Train Epoch: 59 [25600/50000 (51%)]\tLoss: 96.764496\n",
      "Train Epoch: 59 [38400/50000 (77%)]\tLoss: 85.259857\n",
      "Train Epoch: 60 [0/50000 (0%)]\tLoss: 95.205338\n",
      "Train Epoch: 60 [12800/50000 (26%)]\tLoss: 94.329666\n",
      "Train Epoch: 60 [25600/50000 (51%)]\tLoss: 96.128220\n",
      "Train Epoch: 60 [38400/50000 (77%)]\tLoss: 95.774055\n",
      "Train Epoch: 61 [0/50000 (0%)]\tLoss: 95.418121\n",
      "Train Epoch: 61 [12800/50000 (26%)]\tLoss: 93.610672\n",
      "Train Epoch: 61 [25600/50000 (51%)]\tLoss: 93.103165\n",
      "Train Epoch: 61 [38400/50000 (77%)]\tLoss: 93.156021\n",
      "Train Epoch: 62 [0/50000 (0%)]\tLoss: 93.845505\n",
      "Train Epoch: 62 [12800/50000 (26%)]\tLoss: 97.632431\n",
      "Train Epoch: 62 [25600/50000 (51%)]\tLoss: 99.980331\n",
      "Train Epoch: 62 [38400/50000 (77%)]\tLoss: 89.563347\n",
      "Train Epoch: 63 [0/50000 (0%)]\tLoss: 95.881866\n",
      "Train Epoch: 63 [12800/50000 (26%)]\tLoss: 93.659592\n",
      "Train Epoch: 63 [25600/50000 (51%)]\tLoss: 100.945053\n",
      "Train Epoch: 63 [38400/50000 (77%)]\tLoss: 95.725006\n",
      "Train Epoch: 64 [0/50000 (0%)]\tLoss: 92.066681\n",
      "Train Epoch: 64 [12800/50000 (26%)]\tLoss: 92.664459\n",
      "Train Epoch: 64 [25600/50000 (51%)]\tLoss: 96.389290\n",
      "Train Epoch: 64 [38400/50000 (77%)]\tLoss: 98.783081\n",
      "Train Epoch: 65 [0/50000 (0%)]\tLoss: 93.378281\n",
      "Train Epoch: 65 [12800/50000 (26%)]\tLoss: 92.530106\n",
      "Train Epoch: 65 [25600/50000 (51%)]\tLoss: 90.889824\n",
      "Train Epoch: 65 [38400/50000 (77%)]\tLoss: 95.547997\n",
      "Train Epoch: 66 [0/50000 (0%)]\tLoss: 93.639587\n",
      "Train Epoch: 66 [12800/50000 (26%)]\tLoss: 92.158203\n",
      "Train Epoch: 66 [25600/50000 (51%)]\tLoss: 92.613281\n",
      "Train Epoch: 66 [38400/50000 (77%)]\tLoss: 99.828812\n",
      "Train Epoch: 67 [0/50000 (0%)]\tLoss: 94.445587\n",
      "Train Epoch: 67 [12800/50000 (26%)]\tLoss: 93.079575\n",
      "Train Epoch: 67 [25600/50000 (51%)]\tLoss: 93.036995\n",
      "Train Epoch: 67 [38400/50000 (77%)]\tLoss: 94.968742\n",
      "Train Epoch: 68 [0/50000 (0%)]\tLoss: 93.721634\n",
      "Train Epoch: 68 [12800/50000 (26%)]\tLoss: 95.110016\n",
      "Train Epoch: 68 [25600/50000 (51%)]\tLoss: 96.206329\n",
      "Train Epoch: 68 [38400/50000 (77%)]\tLoss: 98.806458\n",
      "Train Epoch: 69 [0/50000 (0%)]\tLoss: 96.682129\n",
      "Train Epoch: 69 [12800/50000 (26%)]\tLoss: 92.296768\n",
      "Train Epoch: 69 [25600/50000 (51%)]\tLoss: 99.917328\n",
      "Train Epoch: 69 [38400/50000 (77%)]\tLoss: 91.372314\n",
      "Train Epoch: 70 [0/50000 (0%)]\tLoss: 91.654572\n",
      "Train Epoch: 70 [12800/50000 (26%)]\tLoss: 94.019127\n",
      "Train Epoch: 70 [25600/50000 (51%)]\tLoss: 91.395004\n",
      "Train Epoch: 70 [38400/50000 (77%)]\tLoss: 94.511177\n",
      "Train Epoch: 71 [0/50000 (0%)]\tLoss: 94.411896\n",
      "Train Epoch: 71 [12800/50000 (26%)]\tLoss: 92.921776\n",
      "Train Epoch: 71 [25600/50000 (51%)]\tLoss: 92.542587\n",
      "Train Epoch: 71 [38400/50000 (77%)]\tLoss: 94.584518\n",
      "Train Epoch: 72 [0/50000 (0%)]\tLoss: 90.962067\n",
      "Train Epoch: 72 [12800/50000 (26%)]\tLoss: 92.871552\n",
      "Train Epoch: 72 [25600/50000 (51%)]\tLoss: 90.778793\n",
      "Train Epoch: 72 [38400/50000 (77%)]\tLoss: 93.845352\n",
      "Train Epoch: 73 [0/50000 (0%)]\tLoss: 95.643196\n",
      "Train Epoch: 73 [12800/50000 (26%)]\tLoss: 93.563751\n",
      "Train Epoch: 73 [25600/50000 (51%)]\tLoss: 94.845543\n",
      "Train Epoch: 73 [38400/50000 (77%)]\tLoss: 94.581352\n",
      "Train Epoch: 74 [0/50000 (0%)]\tLoss: 89.425201\n",
      "Train Epoch: 74 [12800/50000 (26%)]\tLoss: 97.013474\n",
      "Train Epoch: 74 [25600/50000 (51%)]\tLoss: 98.254150\n",
      "Train Epoch: 74 [38400/50000 (77%)]\tLoss: 93.386414\n",
      "Train Epoch: 75 [0/50000 (0%)]\tLoss: 92.098923\n",
      "Train Epoch: 75 [12800/50000 (26%)]\tLoss: 92.152557\n",
      "Train Epoch: 75 [25600/50000 (51%)]\tLoss: 90.528877\n",
      "Train Epoch: 75 [38400/50000 (77%)]\tLoss: 94.830208\n",
      "Train Epoch: 76 [0/50000 (0%)]\tLoss: 91.590858\n",
      "Train Epoch: 76 [12800/50000 (26%)]\tLoss: 91.545334\n",
      "Train Epoch: 76 [25600/50000 (51%)]\tLoss: 92.475494\n",
      "Train Epoch: 76 [38400/50000 (77%)]\tLoss: 91.967690\n",
      "Train Epoch: 77 [0/50000 (0%)]\tLoss: 92.473099\n",
      "Train Epoch: 77 [12800/50000 (26%)]\tLoss: 93.129128\n",
      "Train Epoch: 77 [25600/50000 (51%)]\tLoss: 96.071083\n",
      "Train Epoch: 77 [38400/50000 (77%)]\tLoss: 89.951958\n",
      "Train Epoch: 78 [0/50000 (0%)]\tLoss: 94.016907\n",
      "Train Epoch: 78 [12800/50000 (26%)]\tLoss: 93.079582\n",
      "Train Epoch: 78 [25600/50000 (51%)]\tLoss: 91.750137\n",
      "Train Epoch: 78 [38400/50000 (77%)]\tLoss: 93.447701\n",
      "Train Epoch: 79 [0/50000 (0%)]\tLoss: 92.440788\n",
      "Train Epoch: 79 [12800/50000 (26%)]\tLoss: 93.589180\n",
      "Train Epoch: 79 [25600/50000 (51%)]\tLoss: 92.554924\n",
      "Train Epoch: 79 [38400/50000 (77%)]\tLoss: 90.532654\n",
      "Train Epoch: 80 [0/50000 (0%)]\tLoss: 92.655273\n",
      "Train Epoch: 80 [12800/50000 (26%)]\tLoss: 89.698311\n",
      "Train Epoch: 80 [25600/50000 (51%)]\tLoss: 90.779541\n",
      "Train Epoch: 80 [38400/50000 (77%)]\tLoss: 88.444847\n",
      "Train Epoch: 81 [0/50000 (0%)]\tLoss: 92.742401\n",
      "Train Epoch: 81 [12800/50000 (26%)]\tLoss: 90.260109\n",
      "Train Epoch: 81 [25600/50000 (51%)]\tLoss: 94.105309\n",
      "Train Epoch: 81 [38400/50000 (77%)]\tLoss: 91.380836\n",
      "Train Epoch: 82 [0/50000 (0%)]\tLoss: 89.937973\n",
      "Train Epoch: 82 [12800/50000 (26%)]\tLoss: 93.051849\n",
      "Train Epoch: 82 [25600/50000 (51%)]\tLoss: 89.347481\n",
      "Train Epoch: 82 [38400/50000 (77%)]\tLoss: 90.997208\n",
      "Train Epoch: 83 [0/50000 (0%)]\tLoss: 95.088432\n",
      "Train Epoch: 83 [12800/50000 (26%)]\tLoss: 91.244553\n",
      "Train Epoch: 83 [25600/50000 (51%)]\tLoss: 88.841187\n",
      "Train Epoch: 83 [38400/50000 (77%)]\tLoss: 95.838676\n",
      "Train Epoch: 84 [0/50000 (0%)]\tLoss: 91.525894\n",
      "Train Epoch: 84 [12800/50000 (26%)]\tLoss: 92.205154\n",
      "Train Epoch: 84 [25600/50000 (51%)]\tLoss: 90.470154\n",
      "Train Epoch: 84 [38400/50000 (77%)]\tLoss: 91.182175\n",
      "Train Epoch: 85 [0/50000 (0%)]\tLoss: 86.193802\n",
      "Train Epoch: 85 [12800/50000 (26%)]\tLoss: 97.307121\n",
      "Train Epoch: 85 [25600/50000 (51%)]\tLoss: 92.288284\n",
      "Train Epoch: 85 [38400/50000 (77%)]\tLoss: 91.071373\n",
      "Train Epoch: 86 [0/50000 (0%)]\tLoss: 93.411453\n",
      "Train Epoch: 86 [12800/50000 (26%)]\tLoss: 90.854340\n",
      "Train Epoch: 86 [25600/50000 (51%)]\tLoss: 92.487167\n",
      "Train Epoch: 86 [38400/50000 (77%)]\tLoss: 94.525177\n",
      "Train Epoch: 87 [0/50000 (0%)]\tLoss: 88.087418\n",
      "Train Epoch: 87 [12800/50000 (26%)]\tLoss: 92.349228\n",
      "Train Epoch: 87 [25600/50000 (51%)]\tLoss: 85.629005\n",
      "Train Epoch: 87 [38400/50000 (77%)]\tLoss: 93.807831\n",
      "Train Epoch: 88 [0/50000 (0%)]\tLoss: 91.981659\n",
      "Train Epoch: 88 [12800/50000 (26%)]\tLoss: 92.156990\n",
      "Train Epoch: 88 [25600/50000 (51%)]\tLoss: 95.736893\n",
      "Train Epoch: 88 [38400/50000 (77%)]\tLoss: 96.219688\n",
      "Train Epoch: 89 [0/50000 (0%)]\tLoss: 92.553551\n",
      "Train Epoch: 89 [12800/50000 (26%)]\tLoss: 90.533218\n",
      "Train Epoch: 89 [25600/50000 (51%)]\tLoss: 92.532761\n",
      "Train Epoch: 89 [38400/50000 (77%)]\tLoss: 93.471619\n",
      "Train Epoch: 90 [0/50000 (0%)]\tLoss: 89.888336\n",
      "Train Epoch: 90 [12800/50000 (26%)]\tLoss: 93.458038\n",
      "Train Epoch: 90 [25600/50000 (51%)]\tLoss: 93.038170\n",
      "Train Epoch: 90 [38400/50000 (77%)]\tLoss: 89.680893\n",
      "Train Epoch: 91 [0/50000 (0%)]\tLoss: 90.632385\n",
      "Train Epoch: 91 [12800/50000 (26%)]\tLoss: 90.794144\n",
      "Train Epoch: 91 [25600/50000 (51%)]\tLoss: 92.614143\n",
      "Train Epoch: 91 [38400/50000 (77%)]\tLoss: 90.579430\n",
      "Train Epoch: 92 [0/50000 (0%)]\tLoss: 88.739273\n",
      "Train Epoch: 92 [12800/50000 (26%)]\tLoss: 91.490814\n",
      "Train Epoch: 92 [25600/50000 (51%)]\tLoss: 92.032089\n",
      "Train Epoch: 92 [38400/50000 (77%)]\tLoss: 91.988548\n",
      "Train Epoch: 93 [0/50000 (0%)]\tLoss: 96.229469\n",
      "Train Epoch: 93 [12800/50000 (26%)]\tLoss: 91.998367\n",
      "Train Epoch: 93 [25600/50000 (51%)]\tLoss: 92.567978\n",
      "Train Epoch: 93 [38400/50000 (77%)]\tLoss: 93.900421\n",
      "Train Epoch: 94 [0/50000 (0%)]\tLoss: 90.214470\n",
      "Train Epoch: 94 [12800/50000 (26%)]\tLoss: 92.658417\n",
      "Train Epoch: 94 [25600/50000 (51%)]\tLoss: 94.738113\n",
      "Train Epoch: 94 [38400/50000 (77%)]\tLoss: 96.744598\n",
      "Train Epoch: 95 [0/50000 (0%)]\tLoss: 91.116173\n",
      "Train Epoch: 95 [12800/50000 (26%)]\tLoss: 90.498703\n",
      "Train Epoch: 95 [25600/50000 (51%)]\tLoss: 90.850998\n",
      "Train Epoch: 95 [38400/50000 (77%)]\tLoss: 94.175323\n",
      "Train Epoch: 96 [0/50000 (0%)]\tLoss: 90.466347\n",
      "Train Epoch: 96 [12800/50000 (26%)]\tLoss: 94.525818\n",
      "Train Epoch: 96 [25600/50000 (51%)]\tLoss: 93.066750\n",
      "Train Epoch: 96 [38400/50000 (77%)]\tLoss: 92.783386\n",
      "Train Epoch: 97 [0/50000 (0%)]\tLoss: 89.729576\n",
      "Train Epoch: 97 [12800/50000 (26%)]\tLoss: 95.169678\n",
      "Train Epoch: 97 [25600/50000 (51%)]\tLoss: 94.407867\n",
      "Train Epoch: 97 [38400/50000 (77%)]\tLoss: 93.156204\n",
      "Train Epoch: 98 [0/50000 (0%)]\tLoss: 87.918961\n",
      "Train Epoch: 98 [12800/50000 (26%)]\tLoss: 87.733147\n",
      "Train Epoch: 98 [25600/50000 (51%)]\tLoss: 93.027153\n",
      "Train Epoch: 98 [38400/50000 (77%)]\tLoss: 88.676643\n",
      "Train Epoch: 99 [0/50000 (0%)]\tLoss: 90.939514\n",
      "Train Epoch: 99 [12800/50000 (26%)]\tLoss: 93.274849\n",
      "Train Epoch: 99 [25600/50000 (51%)]\tLoss: 92.494812\n",
      "Train Epoch: 99 [38400/50000 (77%)]\tLoss: 90.927528\n",
      "Train Epoch: 100 [0/50000 (0%)]\tLoss: 94.029587\n",
      "Train Epoch: 100 [12800/50000 (26%)]\tLoss: 90.894142\n",
      "Train Epoch: 100 [25600/50000 (51%)]\tLoss: 93.081253\n",
      "Train Epoch: 100 [38400/50000 (77%)]\tLoss: 91.917107\n",
      "Train Epoch: 101 [0/50000 (0%)]\tLoss: 92.150917\n",
      "Train Epoch: 101 [12800/50000 (26%)]\tLoss: 85.580978\n",
      "Train Epoch: 101 [25600/50000 (51%)]\tLoss: 92.995583\n",
      "Train Epoch: 101 [38400/50000 (77%)]\tLoss: 89.029251\n",
      "Train Epoch: 102 [0/50000 (0%)]\tLoss: 90.163361\n",
      "Train Epoch: 102 [12800/50000 (26%)]\tLoss: 90.780846\n",
      "Train Epoch: 102 [25600/50000 (51%)]\tLoss: 93.149658\n",
      "Train Epoch: 102 [38400/50000 (77%)]\tLoss: 99.211067\n",
      "Train Epoch: 103 [0/50000 (0%)]\tLoss: 96.483551\n",
      "Train Epoch: 103 [12800/50000 (26%)]\tLoss: 89.729675\n",
      "Train Epoch: 103 [25600/50000 (51%)]\tLoss: 92.401550\n",
      "Train Epoch: 103 [38400/50000 (77%)]\tLoss: 93.595131\n",
      "Train Epoch: 104 [0/50000 (0%)]\tLoss: 89.037003\n",
      "Train Epoch: 104 [12800/50000 (26%)]\tLoss: 93.433014\n",
      "Train Epoch: 104 [25600/50000 (51%)]\tLoss: 88.791763\n",
      "Train Epoch: 104 [38400/50000 (77%)]\tLoss: 88.711517\n",
      "Train Epoch: 105 [0/50000 (0%)]\tLoss: 89.367653\n",
      "Train Epoch: 105 [12800/50000 (26%)]\tLoss: 89.994522\n",
      "Train Epoch: 105 [25600/50000 (51%)]\tLoss: 91.792450\n",
      "Train Epoch: 105 [38400/50000 (77%)]\tLoss: 89.073006\n",
      "Train Epoch: 106 [0/50000 (0%)]\tLoss: 89.195374\n",
      "Train Epoch: 106 [12800/50000 (26%)]\tLoss: 88.872757\n",
      "Train Epoch: 106 [25600/50000 (51%)]\tLoss: 87.249680\n",
      "Train Epoch: 106 [38400/50000 (77%)]\tLoss: 94.729073\n",
      "Train Epoch: 107 [0/50000 (0%)]\tLoss: 90.894470\n",
      "Train Epoch: 107 [12800/50000 (26%)]\tLoss: 91.840057\n",
      "Train Epoch: 107 [25600/50000 (51%)]\tLoss: 90.313950\n",
      "Train Epoch: 107 [38400/50000 (77%)]\tLoss: 92.663162\n",
      "Train Epoch: 108 [0/50000 (0%)]\tLoss: 94.873718\n",
      "Train Epoch: 108 [12800/50000 (26%)]\tLoss: 86.713295\n",
      "Train Epoch: 108 [25600/50000 (51%)]\tLoss: 96.523155\n",
      "Train Epoch: 108 [38400/50000 (77%)]\tLoss: 93.327835\n",
      "Train Epoch: 109 [0/50000 (0%)]\tLoss: 87.111015\n",
      "Train Epoch: 109 [12800/50000 (26%)]\tLoss: 90.046776\n",
      "Train Epoch: 109 [25600/50000 (51%)]\tLoss: 90.465096\n",
      "Train Epoch: 109 [38400/50000 (77%)]\tLoss: 96.799850\n",
      "Train Epoch: 110 [0/50000 (0%)]\tLoss: 90.187431\n",
      "Train Epoch: 110 [12800/50000 (26%)]\tLoss: 93.512749\n",
      "Train Epoch: 110 [25600/50000 (51%)]\tLoss: 95.339066\n",
      "Train Epoch: 110 [38400/50000 (77%)]\tLoss: 91.748779\n",
      "Train Epoch: 111 [0/50000 (0%)]\tLoss: 90.974190\n",
      "Train Epoch: 111 [12800/50000 (26%)]\tLoss: 91.717743\n",
      "Train Epoch: 111 [25600/50000 (51%)]\tLoss: 95.432823\n",
      "Train Epoch: 111 [38400/50000 (77%)]\tLoss: 92.522781\n",
      "Train Epoch: 112 [0/50000 (0%)]\tLoss: 94.405823\n",
      "Train Epoch: 112 [12800/50000 (26%)]\tLoss: 91.093292\n",
      "Train Epoch: 112 [25600/50000 (51%)]\tLoss: 96.510651\n",
      "Train Epoch: 112 [38400/50000 (77%)]\tLoss: 90.022446\n",
      "Train Epoch: 113 [0/50000 (0%)]\tLoss: 92.310089\n",
      "Train Epoch: 113 [12800/50000 (26%)]\tLoss: 93.279160\n",
      "Train Epoch: 113 [25600/50000 (51%)]\tLoss: 94.174942\n",
      "Train Epoch: 113 [38400/50000 (77%)]\tLoss: 92.566528\n",
      "Train Epoch: 114 [0/50000 (0%)]\tLoss: 95.093002\n",
      "Train Epoch: 114 [12800/50000 (26%)]\tLoss: 89.667023\n",
      "Train Epoch: 114 [25600/50000 (51%)]\tLoss: 93.781555\n",
      "Train Epoch: 114 [38400/50000 (77%)]\tLoss: 85.690765\n",
      "Train Epoch: 115 [0/50000 (0%)]\tLoss: 91.353012\n",
      "Train Epoch: 115 [12800/50000 (26%)]\tLoss: 92.156982\n",
      "Train Epoch: 115 [25600/50000 (51%)]\tLoss: 88.719406\n",
      "Train Epoch: 115 [38400/50000 (77%)]\tLoss: 90.696121\n",
      "Train Epoch: 116 [0/50000 (0%)]\tLoss: 88.155792\n",
      "Train Epoch: 116 [12800/50000 (26%)]\tLoss: 89.315453\n",
      "Train Epoch: 116 [25600/50000 (51%)]\tLoss: 92.830933\n",
      "Train Epoch: 116 [38400/50000 (77%)]\tLoss: 92.732361\n",
      "Train Epoch: 117 [0/50000 (0%)]\tLoss: 91.695663\n",
      "Train Epoch: 117 [12800/50000 (26%)]\tLoss: 91.120224\n",
      "Train Epoch: 117 [25600/50000 (51%)]\tLoss: 91.140556\n",
      "Train Epoch: 117 [38400/50000 (77%)]\tLoss: 95.019547\n",
      "Train Epoch: 118 [0/50000 (0%)]\tLoss: 89.030823\n",
      "Train Epoch: 118 [12800/50000 (26%)]\tLoss: 90.757278\n",
      "Train Epoch: 118 [25600/50000 (51%)]\tLoss: 90.059433\n",
      "Train Epoch: 118 [38400/50000 (77%)]\tLoss: 93.457550\n",
      "Train Epoch: 119 [0/50000 (0%)]\tLoss: 97.998535\n",
      "Train Epoch: 119 [12800/50000 (26%)]\tLoss: 88.312134\n",
      "Train Epoch: 119 [25600/50000 (51%)]\tLoss: 91.431992\n",
      "Train Epoch: 119 [38400/50000 (77%)]\tLoss: 92.054749\n",
      "Train Epoch: 120 [0/50000 (0%)]\tLoss: 92.215370\n",
      "Train Epoch: 120 [12800/50000 (26%)]\tLoss: 94.743744\n",
      "Train Epoch: 120 [25600/50000 (51%)]\tLoss: 91.634178\n",
      "Train Epoch: 120 [38400/50000 (77%)]\tLoss: 86.961632\n",
      "Train Epoch: 121 [0/50000 (0%)]\tLoss: 90.113632\n",
      "Train Epoch: 121 [12800/50000 (26%)]\tLoss: 91.239349\n",
      "Train Epoch: 121 [25600/50000 (51%)]\tLoss: 93.179390\n",
      "Train Epoch: 121 [38400/50000 (77%)]\tLoss: 92.610146\n",
      "Train Epoch: 122 [0/50000 (0%)]\tLoss: 92.931351\n",
      "Train Epoch: 122 [12800/50000 (26%)]\tLoss: 88.209793\n",
      "Train Epoch: 122 [25600/50000 (51%)]\tLoss: 92.075905\n",
      "Train Epoch: 122 [38400/50000 (77%)]\tLoss: 88.669487\n",
      "Train Epoch: 123 [0/50000 (0%)]\tLoss: 91.258820\n",
      "Train Epoch: 123 [12800/50000 (26%)]\tLoss: 93.615135\n",
      "Train Epoch: 123 [25600/50000 (51%)]\tLoss: 93.066422\n",
      "Train Epoch: 123 [38400/50000 (77%)]\tLoss: 89.372238\n",
      "Train Epoch: 124 [0/50000 (0%)]\tLoss: 94.110558\n",
      "Train Epoch: 124 [12800/50000 (26%)]\tLoss: 92.325760\n",
      "Train Epoch: 124 [25600/50000 (51%)]\tLoss: 92.938354\n",
      "Train Epoch: 124 [38400/50000 (77%)]\tLoss: 94.858650\n",
      "Train Epoch: 125 [0/50000 (0%)]\tLoss: 91.159042\n",
      "Train Epoch: 125 [12800/50000 (26%)]\tLoss: 90.771729\n",
      "Train Epoch: 125 [25600/50000 (51%)]\tLoss: 88.009827\n",
      "Train Epoch: 125 [38400/50000 (77%)]\tLoss: 89.776276\n",
      "Train Epoch: 126 [0/50000 (0%)]\tLoss: 90.115555\n",
      "Train Epoch: 126 [12800/50000 (26%)]\tLoss: 88.935097\n",
      "Train Epoch: 126 [25600/50000 (51%)]\tLoss: 90.354736\n",
      "Train Epoch: 126 [38400/50000 (77%)]\tLoss: 91.119263\n",
      "Train Epoch: 127 [0/50000 (0%)]\tLoss: 93.203323\n",
      "Train Epoch: 127 [12800/50000 (26%)]\tLoss: 92.792038\n",
      "Train Epoch: 127 [25600/50000 (51%)]\tLoss: 94.107910\n",
      "Train Epoch: 127 [38400/50000 (77%)]\tLoss: 92.759308\n",
      "Train Epoch: 128 [0/50000 (0%)]\tLoss: 90.348282\n",
      "Train Epoch: 128 [12800/50000 (26%)]\tLoss: 93.912018\n",
      "Train Epoch: 128 [25600/50000 (51%)]\tLoss: 94.353790\n",
      "Train Epoch: 128 [38400/50000 (77%)]\tLoss: 94.346535\n",
      "Train Epoch: 129 [0/50000 (0%)]\tLoss: 88.357826\n",
      "Train Epoch: 129 [12800/50000 (26%)]\tLoss: 88.890381\n",
      "Train Epoch: 129 [25600/50000 (51%)]\tLoss: 90.217476\n",
      "Train Epoch: 129 [38400/50000 (77%)]\tLoss: 97.554901\n",
      "Train Epoch: 130 [0/50000 (0%)]\tLoss: 92.713928\n",
      "Train Epoch: 130 [12800/50000 (26%)]\tLoss: 90.453934\n",
      "Train Epoch: 130 [25600/50000 (51%)]\tLoss: 89.914680\n",
      "Train Epoch: 130 [38400/50000 (77%)]\tLoss: 92.637009\n",
      "Train Epoch: 131 [0/50000 (0%)]\tLoss: 86.000648\n",
      "Train Epoch: 131 [12800/50000 (26%)]\tLoss: 89.097931\n",
      "Train Epoch: 131 [25600/50000 (51%)]\tLoss: 89.563820\n",
      "Train Epoch: 131 [38400/50000 (77%)]\tLoss: 90.219345\n",
      "Train Epoch: 132 [0/50000 (0%)]\tLoss: 95.695808\n",
      "Train Epoch: 132 [12800/50000 (26%)]\tLoss: 89.836166\n",
      "Train Epoch: 132 [25600/50000 (51%)]\tLoss: 85.973465\n",
      "Train Epoch: 132 [38400/50000 (77%)]\tLoss: 93.174927\n",
      "Train Epoch: 133 [0/50000 (0%)]\tLoss: 88.256004\n",
      "Train Epoch: 133 [12800/50000 (26%)]\tLoss: 89.720901\n",
      "Train Epoch: 133 [25600/50000 (51%)]\tLoss: 89.952881\n",
      "Train Epoch: 133 [38400/50000 (77%)]\tLoss: 90.022896\n",
      "Train Epoch: 134 [0/50000 (0%)]\tLoss: 92.471466\n",
      "Train Epoch: 134 [12800/50000 (26%)]\tLoss: 87.137970\n",
      "Train Epoch: 134 [25600/50000 (51%)]\tLoss: 91.396469\n",
      "Train Epoch: 134 [38400/50000 (77%)]\tLoss: 88.668373\n",
      "Train Epoch: 135 [0/50000 (0%)]\tLoss: 91.329361\n",
      "Train Epoch: 135 [12800/50000 (26%)]\tLoss: 88.442436\n",
      "Train Epoch: 135 [25600/50000 (51%)]\tLoss: 88.176804\n",
      "Train Epoch: 135 [38400/50000 (77%)]\tLoss: 94.704254\n",
      "Train Epoch: 136 [0/50000 (0%)]\tLoss: 89.265732\n",
      "Train Epoch: 136 [12800/50000 (26%)]\tLoss: 92.182106\n",
      "Train Epoch: 136 [25600/50000 (51%)]\tLoss: 89.000389\n",
      "Train Epoch: 136 [38400/50000 (77%)]\tLoss: 90.554939\n",
      "Train Epoch: 137 [0/50000 (0%)]\tLoss: 88.992188\n",
      "Train Epoch: 137 [12800/50000 (26%)]\tLoss: 96.256943\n",
      "Train Epoch: 137 [25600/50000 (51%)]\tLoss: 86.205391\n",
      "Train Epoch: 137 [38400/50000 (77%)]\tLoss: 100.681671\n",
      "Train Epoch: 138 [0/50000 (0%)]\tLoss: 89.501953\n",
      "Train Epoch: 138 [12800/50000 (26%)]\tLoss: 92.538361\n",
      "Train Epoch: 138 [25600/50000 (51%)]\tLoss: 96.290009\n",
      "Train Epoch: 138 [38400/50000 (77%)]\tLoss: 95.565872\n",
      "Train Epoch: 139 [0/50000 (0%)]\tLoss: 87.564758\n",
      "Train Epoch: 139 [12800/50000 (26%)]\tLoss: 97.643097\n",
      "Train Epoch: 139 [25600/50000 (51%)]\tLoss: 90.091431\n",
      "Train Epoch: 139 [38400/50000 (77%)]\tLoss: 91.783203\n",
      "Train Epoch: 140 [0/50000 (0%)]\tLoss: 90.678711\n",
      "Train Epoch: 140 [12800/50000 (26%)]\tLoss: 94.469727\n",
      "Train Epoch: 140 [25600/50000 (51%)]\tLoss: 91.908592\n",
      "Train Epoch: 140 [38400/50000 (77%)]\tLoss: 88.399384\n",
      "Train Epoch: 141 [0/50000 (0%)]\tLoss: 93.777969\n",
      "Train Epoch: 141 [12800/50000 (26%)]\tLoss: 88.414558\n",
      "Train Epoch: 141 [25600/50000 (51%)]\tLoss: 94.848534\n",
      "Train Epoch: 141 [38400/50000 (77%)]\tLoss: 92.678764\n",
      "Train Epoch: 142 [0/50000 (0%)]\tLoss: 92.609283\n",
      "Train Epoch: 142 [12800/50000 (26%)]\tLoss: 90.619781\n",
      "Train Epoch: 142 [25600/50000 (51%)]\tLoss: 92.822845\n",
      "Train Epoch: 142 [38400/50000 (77%)]\tLoss: 88.222885\n",
      "Train Epoch: 143 [0/50000 (0%)]\tLoss: 94.084061\n",
      "Train Epoch: 143 [12800/50000 (26%)]\tLoss: 92.914536\n",
      "Train Epoch: 143 [25600/50000 (51%)]\tLoss: 90.407242\n",
      "Train Epoch: 143 [38400/50000 (77%)]\tLoss: 88.293045\n",
      "Train Epoch: 144 [0/50000 (0%)]\tLoss: 89.845444\n",
      "Train Epoch: 144 [12800/50000 (26%)]\tLoss: 86.775711\n",
      "Train Epoch: 144 [25600/50000 (51%)]\tLoss: 89.134949\n",
      "Train Epoch: 144 [38400/50000 (77%)]\tLoss: 88.349655\n",
      "Train Epoch: 145 [0/50000 (0%)]\tLoss: 89.036331\n",
      "Train Epoch: 145 [12800/50000 (26%)]\tLoss: 84.991219\n",
      "Train Epoch: 145 [25600/50000 (51%)]\tLoss: 89.824944\n",
      "Train Epoch: 145 [38400/50000 (77%)]\tLoss: 89.734436\n",
      "Train Epoch: 146 [0/50000 (0%)]\tLoss: 92.352455\n",
      "Train Epoch: 146 [12800/50000 (26%)]\tLoss: 91.742500\n",
      "Train Epoch: 146 [25600/50000 (51%)]\tLoss: 91.556755\n",
      "Train Epoch: 146 [38400/50000 (77%)]\tLoss: 90.989746\n",
      "Train Epoch: 147 [0/50000 (0%)]\tLoss: 84.889923\n",
      "Train Epoch: 147 [12800/50000 (26%)]\tLoss: 89.376808\n",
      "Train Epoch: 147 [25600/50000 (51%)]\tLoss: 89.085846\n",
      "Train Epoch: 147 [38400/50000 (77%)]\tLoss: 93.921387\n",
      "Train Epoch: 148 [0/50000 (0%)]\tLoss: 90.456093\n",
      "Train Epoch: 148 [12800/50000 (26%)]\tLoss: 92.276909\n",
      "Train Epoch: 148 [25600/50000 (51%)]\tLoss: 90.443832\n",
      "Train Epoch: 148 [38400/50000 (77%)]\tLoss: 86.769539\n",
      "Train Epoch: 149 [0/50000 (0%)]\tLoss: 93.966019\n",
      "Train Epoch: 149 [12800/50000 (26%)]\tLoss: 86.840561\n",
      "Train Epoch: 149 [25600/50000 (51%)]\tLoss: 86.835205\n",
      "Train Epoch: 149 [38400/50000 (77%)]\tLoss: 94.012772\n",
      "Train Epoch: 150 [0/50000 (0%)]\tLoss: 92.062263\n",
      "Train Epoch: 150 [12800/50000 (26%)]\tLoss: 90.063911\n",
      "Train Epoch: 150 [25600/50000 (51%)]\tLoss: 92.395660\n",
      "Train Epoch: 150 [38400/50000 (77%)]\tLoss: 87.975281\n",
      "Train Epoch: 151 [0/50000 (0%)]\tLoss: 88.617455\n",
      "Train Epoch: 151 [12800/50000 (26%)]\tLoss: 89.261948\n",
      "Train Epoch: 151 [25600/50000 (51%)]\tLoss: 87.966629\n",
      "Train Epoch: 151 [38400/50000 (77%)]\tLoss: 91.161621\n",
      "Train Epoch: 152 [0/50000 (0%)]\tLoss: 95.656975\n",
      "Train Epoch: 152 [12800/50000 (26%)]\tLoss: 90.304680\n",
      "Train Epoch: 152 [25600/50000 (51%)]\tLoss: 90.220306\n",
      "Train Epoch: 152 [38400/50000 (77%)]\tLoss: 86.335785\n",
      "Train Epoch: 153 [0/50000 (0%)]\tLoss: 88.100128\n",
      "Train Epoch: 153 [12800/50000 (26%)]\tLoss: 88.137390\n",
      "Train Epoch: 153 [25600/50000 (51%)]\tLoss: 87.546051\n",
      "Train Epoch: 153 [38400/50000 (77%)]\tLoss: 88.966606\n",
      "Train Epoch: 154 [0/50000 (0%)]\tLoss: 90.523911\n",
      "Train Epoch: 154 [12800/50000 (26%)]\tLoss: 90.059502\n",
      "Train Epoch: 154 [25600/50000 (51%)]\tLoss: 87.016296\n",
      "Train Epoch: 154 [38400/50000 (77%)]\tLoss: 89.034241\n",
      "Train Epoch: 155 [0/50000 (0%)]\tLoss: 90.644913\n",
      "Train Epoch: 155 [12800/50000 (26%)]\tLoss: 93.030205\n",
      "Train Epoch: 155 [25600/50000 (51%)]\tLoss: 89.618027\n",
      "Train Epoch: 155 [38400/50000 (77%)]\tLoss: 93.905716\n",
      "Train Epoch: 156 [0/50000 (0%)]\tLoss: 91.224098\n",
      "Train Epoch: 156 [12800/50000 (26%)]\tLoss: 88.859985\n",
      "Train Epoch: 156 [25600/50000 (51%)]\tLoss: 86.660629\n",
      "Train Epoch: 156 [38400/50000 (77%)]\tLoss: 90.773178\n",
      "Train Epoch: 157 [0/50000 (0%)]\tLoss: 93.011497\n",
      "Train Epoch: 157 [12800/50000 (26%)]\tLoss: 89.042885\n",
      "Train Epoch: 157 [25600/50000 (51%)]\tLoss: 89.164520\n",
      "Train Epoch: 157 [38400/50000 (77%)]\tLoss: 90.723099\n",
      "Train Epoch: 158 [0/50000 (0%)]\tLoss: 91.916161\n",
      "Train Epoch: 158 [12800/50000 (26%)]\tLoss: 89.164978\n",
      "Train Epoch: 158 [25600/50000 (51%)]\tLoss: 89.131256\n",
      "Train Epoch: 158 [38400/50000 (77%)]\tLoss: 91.814194\n",
      "Train Epoch: 159 [0/50000 (0%)]\tLoss: 86.138184\n",
      "Train Epoch: 159 [12800/50000 (26%)]\tLoss: 87.829025\n",
      "Train Epoch: 159 [25600/50000 (51%)]\tLoss: 91.614510\n",
      "Train Epoch: 159 [38400/50000 (77%)]\tLoss: 89.234314\n",
      "Train Epoch: 160 [0/50000 (0%)]\tLoss: 91.378174\n",
      "Train Epoch: 160 [12800/50000 (26%)]\tLoss: 92.297417\n",
      "Train Epoch: 160 [25600/50000 (51%)]\tLoss: 89.614731\n",
      "Train Epoch: 160 [38400/50000 (77%)]\tLoss: 87.939758\n",
      "Train Epoch: 161 [0/50000 (0%)]\tLoss: 90.045494\n",
      "Train Epoch: 161 [12800/50000 (26%)]\tLoss: 90.978470\n",
      "Train Epoch: 161 [25600/50000 (51%)]\tLoss: 85.410843\n",
      "Train Epoch: 161 [38400/50000 (77%)]\tLoss: 93.226105\n",
      "Train Epoch: 162 [0/50000 (0%)]\tLoss: 88.724976\n",
      "Train Epoch: 162 [12800/50000 (26%)]\tLoss: 91.586700\n",
      "Train Epoch: 162 [25600/50000 (51%)]\tLoss: 90.626076\n",
      "Train Epoch: 162 [38400/50000 (77%)]\tLoss: 89.463867\n",
      "Train Epoch: 163 [0/50000 (0%)]\tLoss: 91.595955\n",
      "Train Epoch: 163 [12800/50000 (26%)]\tLoss: 91.539047\n",
      "Train Epoch: 163 [25600/50000 (51%)]\tLoss: 93.847122\n",
      "Train Epoch: 163 [38400/50000 (77%)]\tLoss: 89.883408\n",
      "Train Epoch: 164 [0/50000 (0%)]\tLoss: 93.362061\n",
      "Train Epoch: 164 [12800/50000 (26%)]\tLoss: 89.039558\n",
      "Train Epoch: 164 [25600/50000 (51%)]\tLoss: 95.131676\n",
      "Train Epoch: 164 [38400/50000 (77%)]\tLoss: 90.252213\n",
      "Train Epoch: 165 [0/50000 (0%)]\tLoss: 88.940811\n",
      "Train Epoch: 165 [12800/50000 (26%)]\tLoss: 89.163078\n",
      "Train Epoch: 165 [25600/50000 (51%)]\tLoss: 86.673790\n",
      "Train Epoch: 165 [38400/50000 (77%)]\tLoss: 90.940170\n",
      "Train Epoch: 166 [0/50000 (0%)]\tLoss: 92.440430\n",
      "Train Epoch: 166 [12800/50000 (26%)]\tLoss: 91.272499\n",
      "Train Epoch: 166 [25600/50000 (51%)]\tLoss: 89.412369\n",
      "Train Epoch: 166 [38400/50000 (77%)]\tLoss: 90.049660\n",
      "Train Epoch: 167 [0/50000 (0%)]\tLoss: 93.352783\n",
      "Train Epoch: 167 [12800/50000 (26%)]\tLoss: 88.934067\n",
      "Train Epoch: 167 [25600/50000 (51%)]\tLoss: 95.236397\n",
      "Train Epoch: 167 [38400/50000 (77%)]\tLoss: 94.522415\n",
      "Train Epoch: 168 [0/50000 (0%)]\tLoss: 94.234314\n",
      "Train Epoch: 168 [12800/50000 (26%)]\tLoss: 95.564209\n",
      "Train Epoch: 168 [25600/50000 (51%)]\tLoss: 93.214653\n",
      "Train Epoch: 168 [38400/50000 (77%)]\tLoss: 90.182800\n",
      "Train Epoch: 169 [0/50000 (0%)]\tLoss: 88.184982\n",
      "Train Epoch: 169 [12800/50000 (26%)]\tLoss: 91.008484\n",
      "Train Epoch: 169 [25600/50000 (51%)]\tLoss: 88.836853\n",
      "Train Epoch: 169 [38400/50000 (77%)]\tLoss: 92.865196\n",
      "Train Epoch: 170 [0/50000 (0%)]\tLoss: 84.603561\n",
      "Train Epoch: 170 [12800/50000 (26%)]\tLoss: 90.889496\n",
      "Train Epoch: 170 [25600/50000 (51%)]\tLoss: 85.576614\n",
      "Train Epoch: 170 [38400/50000 (77%)]\tLoss: 87.808823\n",
      "Train Epoch: 171 [0/50000 (0%)]\tLoss: 88.065826\n",
      "Train Epoch: 171 [12800/50000 (26%)]\tLoss: 84.218262\n",
      "Train Epoch: 171 [25600/50000 (51%)]\tLoss: 87.475815\n",
      "Train Epoch: 171 [38400/50000 (77%)]\tLoss: 86.475250\n",
      "Train Epoch: 172 [0/50000 (0%)]\tLoss: 89.229904\n",
      "Train Epoch: 172 [12800/50000 (26%)]\tLoss: 89.558678\n",
      "Train Epoch: 172 [25600/50000 (51%)]\tLoss: 97.269531\n",
      "Train Epoch: 172 [38400/50000 (77%)]\tLoss: 91.429596\n",
      "Train Epoch: 173 [0/50000 (0%)]\tLoss: 91.980263\n",
      "Train Epoch: 173 [12800/50000 (26%)]\tLoss: 87.050819\n",
      "Train Epoch: 173 [25600/50000 (51%)]\tLoss: 92.422165\n",
      "Train Epoch: 173 [38400/50000 (77%)]\tLoss: 93.121414\n",
      "Train Epoch: 174 [0/50000 (0%)]\tLoss: 84.922943\n",
      "Train Epoch: 174 [12800/50000 (26%)]\tLoss: 93.292969\n",
      "Train Epoch: 174 [25600/50000 (51%)]\tLoss: 94.556396\n",
      "Train Epoch: 174 [38400/50000 (77%)]\tLoss: 90.113281\n",
      "Train Epoch: 175 [0/50000 (0%)]\tLoss: 90.139496\n",
      "Train Epoch: 175 [12800/50000 (26%)]\tLoss: 95.071709\n",
      "Train Epoch: 175 [25600/50000 (51%)]\tLoss: 90.524948\n",
      "Train Epoch: 175 [38400/50000 (77%)]\tLoss: 88.620621\n",
      "Train Epoch: 176 [0/50000 (0%)]\tLoss: 86.757042\n",
      "Train Epoch: 176 [12800/50000 (26%)]\tLoss: 90.695419\n",
      "Train Epoch: 176 [25600/50000 (51%)]\tLoss: 88.333160\n",
      "Train Epoch: 176 [38400/50000 (77%)]\tLoss: 88.297562\n",
      "Train Epoch: 177 [0/50000 (0%)]\tLoss: 92.048996\n",
      "Train Epoch: 177 [12800/50000 (26%)]\tLoss: 87.840187\n",
      "Train Epoch: 177 [25600/50000 (51%)]\tLoss: 86.812492\n",
      "Train Epoch: 177 [38400/50000 (77%)]\tLoss: 87.705643\n",
      "Train Epoch: 178 [0/50000 (0%)]\tLoss: 88.982101\n",
      "Train Epoch: 178 [12800/50000 (26%)]\tLoss: 90.452522\n",
      "Train Epoch: 178 [25600/50000 (51%)]\tLoss: 88.852882\n",
      "Train Epoch: 178 [38400/50000 (77%)]\tLoss: 90.489677\n",
      "Train Epoch: 179 [0/50000 (0%)]\tLoss: 92.685867\n",
      "Train Epoch: 179 [12800/50000 (26%)]\tLoss: 88.741661\n",
      "Train Epoch: 179 [25600/50000 (51%)]\tLoss: 90.724960\n",
      "Train Epoch: 179 [38400/50000 (77%)]\tLoss: 85.231041\n",
      "Train Epoch: 180 [0/50000 (0%)]\tLoss: 92.156967\n",
      "Train Epoch: 180 [12800/50000 (26%)]\tLoss: 93.409348\n",
      "Train Epoch: 180 [25600/50000 (51%)]\tLoss: 90.514549\n",
      "Train Epoch: 180 [38400/50000 (77%)]\tLoss: 91.170189\n",
      "Train Epoch: 181 [0/50000 (0%)]\tLoss: 85.015900\n",
      "Train Epoch: 181 [12800/50000 (26%)]\tLoss: 91.676422\n",
      "Train Epoch: 181 [25600/50000 (51%)]\tLoss: 85.929550\n",
      "Train Epoch: 181 [38400/50000 (77%)]\tLoss: 94.005524\n",
      "Train Epoch: 182 [0/50000 (0%)]\tLoss: 90.801361\n",
      "Train Epoch: 182 [12800/50000 (26%)]\tLoss: 92.979660\n",
      "Train Epoch: 182 [25600/50000 (51%)]\tLoss: 88.618866\n",
      "Train Epoch: 182 [38400/50000 (77%)]\tLoss: 93.210793\n",
      "Train Epoch: 183 [0/50000 (0%)]\tLoss: 94.178894\n",
      "Train Epoch: 183 [12800/50000 (26%)]\tLoss: 85.665703\n",
      "Train Epoch: 183 [25600/50000 (51%)]\tLoss: 87.638664\n",
      "Train Epoch: 183 [38400/50000 (77%)]\tLoss: 94.223648\n",
      "Train Epoch: 184 [0/50000 (0%)]\tLoss: 92.275482\n",
      "Train Epoch: 184 [12800/50000 (26%)]\tLoss: 88.308762\n",
      "Train Epoch: 184 [25600/50000 (51%)]\tLoss: 86.047829\n",
      "Train Epoch: 184 [38400/50000 (77%)]\tLoss: 88.660614\n",
      "Train Epoch: 185 [0/50000 (0%)]\tLoss: 93.213127\n",
      "Train Epoch: 185 [12800/50000 (26%)]\tLoss: 91.912933\n",
      "Train Epoch: 185 [25600/50000 (51%)]\tLoss: 91.831604\n",
      "Train Epoch: 185 [38400/50000 (77%)]\tLoss: 85.102097\n",
      "Train Epoch: 186 [0/50000 (0%)]\tLoss: 88.972626\n",
      "Train Epoch: 186 [12800/50000 (26%)]\tLoss: 89.035645\n",
      "Train Epoch: 186 [25600/50000 (51%)]\tLoss: 87.539879\n",
      "Train Epoch: 186 [38400/50000 (77%)]\tLoss: 91.218140\n",
      "Train Epoch: 187 [0/50000 (0%)]\tLoss: 89.532043\n",
      "Train Epoch: 187 [12800/50000 (26%)]\tLoss: 89.063339\n",
      "Train Epoch: 187 [25600/50000 (51%)]\tLoss: 88.608566\n",
      "Train Epoch: 187 [38400/50000 (77%)]\tLoss: 91.212898\n",
      "Train Epoch: 188 [0/50000 (0%)]\tLoss: 87.608376\n",
      "Train Epoch: 188 [12800/50000 (26%)]\tLoss: 90.994728\n",
      "Train Epoch: 188 [25600/50000 (51%)]\tLoss: 92.203369\n",
      "Train Epoch: 188 [38400/50000 (77%)]\tLoss: 88.912544\n",
      "Train Epoch: 189 [0/50000 (0%)]\tLoss: 86.846405\n",
      "Train Epoch: 189 [12800/50000 (26%)]\tLoss: 93.760040\n",
      "Train Epoch: 189 [25600/50000 (51%)]\tLoss: 89.947578\n",
      "Train Epoch: 189 [38400/50000 (77%)]\tLoss: 87.739410\n",
      "Train Epoch: 190 [0/50000 (0%)]\tLoss: 91.632950\n",
      "Train Epoch: 190 [12800/50000 (26%)]\tLoss: 92.221420\n",
      "Train Epoch: 190 [25600/50000 (51%)]\tLoss: 86.701157\n",
      "Train Epoch: 190 [38400/50000 (77%)]\tLoss: 92.164795\n",
      "Train Epoch: 191 [0/50000 (0%)]\tLoss: 88.307144\n",
      "Train Epoch: 191 [12800/50000 (26%)]\tLoss: 88.634308\n",
      "Train Epoch: 191 [25600/50000 (51%)]\tLoss: 88.292618\n",
      "Train Epoch: 191 [38400/50000 (77%)]\tLoss: 93.542877\n",
      "Train Epoch: 192 [0/50000 (0%)]\tLoss: 88.053528\n",
      "Train Epoch: 192 [12800/50000 (26%)]\tLoss: 87.415291\n",
      "Train Epoch: 192 [25600/50000 (51%)]\tLoss: 88.090515\n",
      "Train Epoch: 192 [38400/50000 (77%)]\tLoss: 92.270760\n",
      "Train Epoch: 193 [0/50000 (0%)]\tLoss: 92.091873\n",
      "Train Epoch: 193 [12800/50000 (26%)]\tLoss: 88.095474\n",
      "Train Epoch: 193 [25600/50000 (51%)]\tLoss: 87.734756\n",
      "Train Epoch: 193 [38400/50000 (77%)]\tLoss: 93.227768\n",
      "Train Epoch: 194 [0/50000 (0%)]\tLoss: 90.492256\n",
      "Train Epoch: 194 [12800/50000 (26%)]\tLoss: 90.785683\n",
      "Train Epoch: 194 [25600/50000 (51%)]\tLoss: 91.878220\n",
      "Train Epoch: 194 [38400/50000 (77%)]\tLoss: 89.151810\n",
      "Train Epoch: 195 [0/50000 (0%)]\tLoss: 89.978851\n",
      "Train Epoch: 195 [12800/50000 (26%)]\tLoss: 93.025818\n",
      "Train Epoch: 195 [25600/50000 (51%)]\tLoss: 88.624985\n",
      "Train Epoch: 195 [38400/50000 (77%)]\tLoss: 93.043022\n",
      "Train Epoch: 196 [0/50000 (0%)]\tLoss: 87.598175\n",
      "Train Epoch: 196 [12800/50000 (26%)]\tLoss: 89.002174\n",
      "Train Epoch: 196 [25600/50000 (51%)]\tLoss: 87.900375\n",
      "Train Epoch: 196 [38400/50000 (77%)]\tLoss: 88.218277\n",
      "Train Epoch: 197 [0/50000 (0%)]\tLoss: 91.305771\n",
      "Train Epoch: 197 [12800/50000 (26%)]\tLoss: 87.733200\n",
      "Train Epoch: 197 [25600/50000 (51%)]\tLoss: 94.938095\n",
      "Train Epoch: 197 [38400/50000 (77%)]\tLoss: 92.075813\n",
      "Train Epoch: 198 [0/50000 (0%)]\tLoss: 88.186478\n",
      "Train Epoch: 198 [12800/50000 (26%)]\tLoss: 88.032372\n",
      "Train Epoch: 198 [25600/50000 (51%)]\tLoss: 88.995720\n",
      "Train Epoch: 198 [38400/50000 (77%)]\tLoss: 86.986237\n",
      "Train Epoch: 199 [0/50000 (0%)]\tLoss: 90.513641\n",
      "Train Epoch: 199 [12800/50000 (26%)]\tLoss: 87.484001\n",
      "Train Epoch: 199 [25600/50000 (51%)]\tLoss: 89.879105\n",
      "Train Epoch: 199 [38400/50000 (77%)]\tLoss: 85.570267\n",
      "Train Epoch: 200 [0/50000 (0%)]\tLoss: 85.658218\n",
      "Train Epoch: 200 [12800/50000 (26%)]\tLoss: 89.170532\n",
      "Train Epoch: 200 [25600/50000 (51%)]\tLoss: 89.277466\n",
      "Train Epoch: 200 [38400/50000 (77%)]\tLoss: 90.563965\n"
     ]
    }
   ],
   "source": [
    "train_vae(epochs=200, batch_size=128, learning_rate=1e-3, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_display_samples(log_dir, epoch=100, num_samples=64, latent_dims=[512, 256, 128], device='cuda'):\n",
    "    \"\"\"Generate and display random samples from the VAE decoder\"\"\"\n",
    "    model = HierarchicalVAE(latent_dims=latent_dims).to(device)\n",
    "    checkpoint = torch.load(f'{log_dir}/models/hvae_checkpoint_epoch_{str(epoch)}.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = model.random_samples(num_samples, device)\n",
    "        \n",
    "        # Create a grid of images\n",
    "        fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "            # Convert from [C,H,W] to [H,W,C] format\n",
    "            img = samples[idx].permute(1, 2, 0)\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model instance\n",
    "log_dir = './runs/HVAE_CIFAR10_20241227-204102'\n",
    "samples = generate_and_display_samples(log_dir, 200)\n",
    "\n",
    "# Optionally save to disk\n",
    "torchvision.utils.save_image(samples, 'vae_samples.png', nrow=8, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
